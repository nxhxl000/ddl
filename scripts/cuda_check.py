import sys
import traceback

def main() -> int:
    try:
        import torch
    except Exception as e:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å torch:", repr(e))
        return 1

    print("=== PyTorch / CUDA environment check ===")
    print(f"Python: {sys.version.split()[0]}")
    print(f"Torch version: {torch.__version__}")
    print(f"CUDA compiled (torch.version.cuda): {torch.version.cuda}")
    print(f"cuDNN version: {torch.backends.cudnn.version()}")
    print(f"cuDNN enabled: {torch.backends.cudnn.enabled}")

    # –ë–∞–∑–æ–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
    cuda_available = torch.cuda.is_available()
    print(f"CUDA available (torch.cuda.is_available): {cuda_available}")

    # –ï—Å–ª–∏ CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º —Å –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏
    if not cuda_available:
        print("\n‚ö†Ô∏è CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞.")
        print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("- –£ —Ç–µ–±—è –Ω–µ—Ç NVIDIA GPU –∏–ª–∏ —Ç—ã –Ω–µ –≤ —Å—Ä–µ–¥–µ —Å –ø—Ä–æ–±—Ä–æ—à–µ–Ω–Ω–æ–π GPU (WSL –±–µ–∑ GPU, –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –±–µ–∑ --gpus, etc.)")
        print("- –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω/–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç NVIDIA –¥—Ä–∞–π–≤–µ—Ä (–ø—Ä–æ–≤–µ—Ä—å nvidia-smi)")
        print("- –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã CPU wheels PyTorch –≤–º–µ—Å—Ç–æ CUDA wheels")
        return 2

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    try:
        n = torch.cuda.device_count()
        print(f"CUDA device count: {n}")
        for i in range(n):
            props = torch.cuda.get_device_properties(i)
            print(f"  [{i}] {props.name} | {props.total_memory/1024**3:.2f} GB | CC {props.major}.{props.minor}")
    except Exception:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–≤–æ–π—Å—Ç–≤–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.")
        traceback.print_exc()

    # –¢–µ—Å—Ç 1: –ø—Ä–æ—Å—Ç–∞—è —Ç–µ–Ω–∑–æ—Ä–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–∞ GPU
    try:
        device = torch.device("cuda:0")
        x = torch.randn(1024, 1024, device=device)
        y = torch.randn(1024, 1024, device=device)
        z = x @ y
        torch.cuda.synchronize()
        print("\n‚úÖ GPU matmul OK. Result:", float(z[0, 0]))
    except Exception:
        print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ GPU matmul.")
        traceback.print_exc()
        return 3

    # –¢–µ—Å—Ç 2: –Ω–µ–±–æ–ª—å—à–æ–π Conv2D (–ø—Ä–æ–≤–µ—Ä–∫–∞ cuDNN)
    try:
        import torch.nn as nn
        conv = nn.Conv2d(3, 16, kernel_size=3, padding=1).to(device)
        inp = torch.randn(8, 3, 64, 64, device=device)
        out = conv(inp)
        loss = out.mean()
        loss.backward()
        torch.cuda.synchronize()
        print("‚úÖ cuDNN/Conv2d forward+backward OK. out shape:", tuple(out.shape))
    except Exception:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ Conv2d/cuDNN —Ç–µ—Å—Ç–µ.")
        traceback.print_exc()
        return 4

    # –ò–º–ø–æ—Ä—Ç torchvision/torchaudio (–µ—Å–ª–∏ —Å—Ç–æ—è—Ç)
    for pkg in ("torchvision", "torchaudio"):
        try:
            mod = __import__(pkg)
            ver = getattr(mod, "__version__", "unknown")
            print(f"‚úÖ {pkg} import OK. version: {ver}")
        except Exception as e:
            print(f"‚ö†Ô∏è {pkg} –∏–º–ø–æ—Ä—Ç –Ω–µ —É–¥–∞–ª—Å—è: {repr(e)}")

    print("\nüéâ –í—Å—ë –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ: CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ PyTorch –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞ GPU.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())